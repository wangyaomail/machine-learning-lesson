# 集成学习

## 请阐述组合模型的好处。
答案：
1)	从统计的角度说，可以使多个假设在训练集上达到同等的性能，减小个别学习器泛化性能不好的情况。
2)	从计算上说，学习算法容易陷入局部极小，多次结合可以降低陷入局部极小的风险。
3)	从表示的方面说，某些学习任务的真实假设可能不在当前学习算法的假设空间内。
## 请说明集成学习中Boosting算法的原理和典型应用。
答案：
1)	Boosting算法通过调整每一轮的数据权重来生成新的数据分布。
2)	数据权重变化后，可以在重采样中提升高权重数据的采样概率，也可以在计算损失时提升高权重数据的损失值。
3)	Boosting算法中的典型算法是AdaBoost。

## 请对比并说明集成学习中Boosting和Bagging的典型算法和异同。
答案：
1)	数据上：Boosting在训练过程中通过调整每一轮的数据权重实现对上一轮分错数据的加权，Bagging在训练过程中通过重采样生成不同的训练集。
2)	计算上：Boosting使用各个基学习器结果的加权和，Bagging则使用投票法。
3)	速度上：Boosting的训练和计算在学习器之间有依赖关系，难以并行因此速率较低。Bagging各个学习器之间无依赖，因此速率较高。
4)	典型算法：Boosting的典型算法是AdaBoost，Bagging的典型算法是随机森林。

## 集成学习中基学习器越多越好吗？为什么？
答案：
1)	基学习器相互独立时，越多越好，反之如果基学习器相关性较高，则提高数量并不一定能提高准确度。
2)	基学习器各自的错误率较低时，越多越好，反之如果各个基学习器错误率接近或超过50%，组合后不一定能提高集成学习器的准确度。
## 请简述Boosting算法的工作机制？
答案：
1)	从初始训练集上训练一个基学习器。
2)	根据该基学习器对训练样本分布进行调整，使得先前分类错的样本得到更多关注。
3)	基于调整后样本分布训练下一个基学习器。
4)	使用集成模型的结合策略组合各个基学习器的结果。
## 请简述Bagging算法的工作机制？
答案：
1)	使用重采样或重赋权在原数据集上生成一系列不饱和数据集。
2)	在每个不饱和数据集上训练出一个基学习器。
3)	使用集成模型的结合策略组合各个基学习器的结果。
## 请简述集成学习的结合策略，并简要说明？
答案：
1)	平均法，包括简单平均和加权平均。
2)	投票法，包括绝对多数投票、相对多数投票和加权投票。
3)	stacking法，以各个基学习器为特征训练一个学习器作为结果。
## 请简述集成学习的主要特点。
答案：
1)	模型的泛化能力的强，减少过拟合和欠拟合的风险。
2)	预测性能好，由于集成学习可以组合多个弱学习器，所以它可以将多个模型的优点进行整合，从而提高整体的预测性能。
3)	鲁棒性强，对于噪音和脏数据的容错性高。
4)	可扩展性强，由于集成学习可以将多个模型进行组合，所以它可以方便地扩展到更大的数据集和更复杂的模型。
5)	可解释性好，由于集成学习的模型比较复杂，所以它的可解释性比较差，难以理解和解释模型的预测结果。